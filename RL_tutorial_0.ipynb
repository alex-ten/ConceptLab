{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Q-learning (table lookup approach)\n",
    "\n",
    "In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.\n",
    "\n",
    "Bellman equation: $ Q(s,a) = r + \\gamma(max(Q(s’,a’)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def choose_Q_act(Q_table, state, policy, noise, noise_attenuation):\n",
    "    num_acts = len(Q_table[state,:])\n",
    "    return policy(Q_table[state,:] + noise(1,num_acts)*noise_attenuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with Frozen Lake\n",
    "TODO: make an interactive widget based app with four buttons and a square 4x4 display\n",
    "\n",
    "Features:\n",
    "- Each step should leave a line trace, older traces being fainter than the new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_env = gym.make('FrozenLake-v0')\n",
    "_ = i_env.reset()\n",
    "while True:\n",
    "    x = input('Enter to random step, q to quit')\n",
    "    if x!='q':\n",
    "        i_env.step(np.random.randint(4))\n",
    "        i_env.render()\n",
    "    else: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Q-learning\n",
    "Based on the app above, create a visual interactive Q-learner. One can click through simulated learning algorithm and see how the initially empty Q-table gets updated. Each step should result in an update of the environment and several indicators (e.g. whether the step was successful).\n",
    "\n",
    "To visualize Q-learning, plot the current Q-table as well as the current update that results from a certain action and its success: \n",
    "\n",
    "```python\n",
    "np.max(Q[new_s,:]) - Q[s,a]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "ospace = env.observation_space\n",
    "aspace = env.action_space\n",
    "actions = ['L', 'D', 'R', 'U'] # left, down, right, up\n",
    "\n",
    "# Initialize table with all zeros\n",
    "Q = np.zeros([ospace.n, aspace.n])\n",
    "\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "gamma = .95\n",
    "num_epochs = 1000\n",
    "num_steps = 99\n",
    "\n",
    "# Create empty lists to contain total rewards (returns) and steps per episode\n",
    "data_steps = []\n",
    "data_returns = []\n",
    "data_acts = []\n",
    "data_Q = [Q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Begin epoch\n",
    "    s = env.reset() # reset env and get init state\n",
    "    R = 0 # init reward is 0\n",
    "    d = False # no action is successful\n",
    "    j = 0 # no learning steps commited\n",
    "    \n",
    "    # The Q-Table learning algorithm\n",
    "    for j in range(num_steps):\n",
    "        \n",
    "        # Choose an action by greedily (with noise) picking from Q table (see def Q_act above)\n",
    "        a = choose_Q_act(\n",
    "            Q_table = Q,\n",
    "            policy = np.argmax,\n",
    "            state = s,\n",
    "            noise = np.random.randn,\n",
    "            noise_attenuation = 1./(i+1)\n",
    "        )\n",
    "        \n",
    "        # Act and get new state and reward from environment\n",
    "        new_s,r,d,_ = env.step(a)\n",
    "        \n",
    "        # Update Q-Table with new knowledge and transition to new state\n",
    "        Q[s,a] = Q[s,a] + lr*(r + gamma*np.max(Q[new_s,:]) - Q[s,a])\n",
    "        s = new_s\n",
    "        \n",
    "        # Accumulate return\n",
    "        R += r\n",
    "        \n",
    "        # Store data\n",
    "        data_acts.append(a)\n",
    "        if d == True:\n",
    "            data_steps.append(j)\n",
    "            break\n",
    "            \n",
    "    data_returns.append(R)\n",
    "    data_Q.append(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# slide = widgets.IntSlider(min=0, max=len(data_Q), description='epoch')\n",
    "\n",
    "# im = plt.imshow(data_Q[0])\n",
    "\n",
    "# def plot_Q(i, im, data):\n",
    "#     im.set_data(data[i])\n",
    "#     plt.gcf().canvas.draw_idle()\n",
    "\n",
    "# widgets.interact(plot_Q, i=slide, im=widgets.fixed(im), data=widgets.fixed(data_Q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
